{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Unusual meaning map: Treating question pairs as image / surface\n---------------------------------------------------------------",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Other people have already written really nice exploratory kernels which helped me to write the minimal code myself. \n\nIn this kernel, I have tried to extract a different type of feature from which we can learn using any algorithm which can learn via image. The basic assumption behind this exercise is to capture non-sequential closeness between words.\n\nFor example:\nA Question pair has pointing arrows from each of the words of one sentence to each of the words from another sentence\n![A Question pair has pointing arrows from each of the words of one sentence to each of the words from another sentence][1]\n\n  [1]: http://image.prntscr.com/image/97e92b0357a843078b61eef5ad8a183b.png\n\nTo capture this we can create NxM matrix with Word2Vec distance between each word with other. and resize the matrix just like an image to a 10x10 matrix and use this as a feature to xgboost.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv\nimport pip\nfrom gensim import corpora, models, similarities\nimport pandas as pd\nimport numpy as np\ntrain_file = \"../input/train.csv\"\ndf = pd.read_csv(train_file, index_col=\"id\")\ndf",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pylab as plt",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Extracting unique questions**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "questions = dict()\n\nfor row in df.iterrows():\n    questions[row[1]['qid1']] = row[1]['question1']\n    questions[row[1]['qid2']] = row[1]['question2']",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Creating a simple tokenizer**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import re\nimport nltk\ndef basic_cleaning(string):\n    string = str(string)\n    try:\n        string = string.decode('unicode-escape')\n    except Exception:\n        pass\n    string = string.lower()\n    string = re.sub(' +', ' ', string)\n    return string\nsentences = []\nfor i in questions:\n    sentences.append(nltk.word_tokenize(basic_cleaning(questions[i])))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Creating a simple Word2Vec model from the question pair, we can use a pre-trained model instead to get better results**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import gensim\nmodel = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**A very simple term frequency and document frequency extractor** ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tf = dict()\ndocf = dict()\ntotal_docs = 0\nfor qid in questions:\n    total_docs += 1\n    toks = nltk.word_tokenize(basic_cleaning(questions[qid]))\n    uniq_toks = set(toks)\n    for i in toks:\n        if i not in tf:\n            tf[i] = 1\n        else:\n            tf[i] += 1\n    for i in uniq_toks:\n        if i not in docf:\n            docf[i] = 1\n        else:\n            docf[i] += 1",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Mimic the IDF function but penalize the words which have fairly high score otherwise, and give a strong boost to the words which appear sporadically.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from __future__ import division\nimport math\ndef idf(word):\n    return 1 - math.sqrt(docf[word]/total_docs)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(idf(\"kenya\"))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A simple cleaning module for feature extraction",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import re\nimport nltk\ndef basic_cleaning(string):\n    string = str(string)\n    string = string.lower()\n    string = re.sub('[0-9\\(\\)\\!\\^\\%\\$\\'\\\"\\.;,-\\?\\{\\}\\[\\]\\\\/]', ' ', string)\n    string = ' '.join([i for i in string.split() if i not in [\"a\", \"and\", \"of\", \"the\", \"to\", \"on\", \"in\", \"at\", \"is\"]])\n    string = re.sub(' +', ' ', string)\n    return string",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def w2v_sim(w1, w2):\n    try:\n        return model.similarity(w1, w2)*idf(w1)*idf(w2)\n    except Exception:\n        return 0.0",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Visualizing features**\n\nThis function will create a 10x10 matrix using MxN word pairs among the words of question pair",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\nimport matplotlib.cm as cm\nfrom scipy import *\ndf = df.sample(n=30000)\ndef imagify(row):\n    s1 = row['question1']\n    s2 = row['question2']\n    t1 = list((basic_cleaning(s1)).split())\n    t2 = list((basic_cleaning(s2)).split())\n    print(\"Q1: \"+ s1)\n    print(\"Q2: \"+ s2)\n    print(\"Duplicate: \" + str(row['is_duplicate']))\n    \n    img = [[w2v_sim(x, y) for x in t1] for y in t2] \n    a = np.array(img, order='C')\n    img = np.resize(a,(10,10))\n    # print img\n    fig = plt.figure()\n    # tell imshow about color map so that only set colors are used\n    image = plt.imshow(img,interpolation='nearest')\n    # make a color bar\n    plt.colorbar(image)\n    plt.show()\ns = df.sample(n=3)\nplt.close()\ns.apply(imagify, axis=1, raw=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nplt.close()\ndef surface(row):\n    s1 = row['question1']\n    s2 = row['question2']\n    t1 = list((basic_cleaning(s1)).split())\n    t2 = list((basic_cleaning(s2)).split())\n    print(\"Q1: \"+ s1)\n    print(\"Q2: \"+ s2)\n    print(\"Duplicate: \" + str(row['is_duplicate']))\n    \n#     img = [[w2v_sim(x, y) for x in t1] for y in t2] \n\n    fig = plt.figure()\n    ax = Axes3D(fig)\n    X = linspace(0,10,10)\n    Y = linspace(0,10,10)\n    X, Y = meshgrid(X, Y)\n    Z = [[w2v_sim(x, y) for x in t1] for y in t2] \n    a = np.array(Z, order='C')\n    Z = np.resize(a,(10,10))\n    \n    ax.plot_surface(Y, X, Z, rstride=1, cstride=1, cmap=cm.jet)\n    ax.set_xlabel(\"X Axis\")\n    ax.set_ylabel(\"Y Axis\")\n    ax.set_zlabel(\"Z Axis\")\n    plt.show()\n    \ns = df.sample(n=3)\nplt.close()\ns.apply(surface, axis=1, raw=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def img_feature(row):\n    s1 = row['question1']\n    s2 = row['question2']\n    t1 = list((basic_cleaning(s1)).split())\n    t2 = list((basic_cleaning(s2)).split())\n    Z = [[w2v_sim(x, y) for x in t1] for y in t2] \n    a = np.array(Z, order='C')\n    return [np.resize(a,(10,10)).flatten()]\ns = df\n\nimg = s.apply(img_feature, axis=1, raw=True)\npix_col = [[] for y in range(100)] \nfor k in img.iteritems():\n        for f in range(len(list(k[1][0]))):\n           pix_col[f].append(k[1][0][f])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Extracting Features**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom __future__ import division\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\ntrain_word_match = df.apply(word_match_share, axis=1, raw=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from __future__ import division\nx_train = pd.DataFrame()\n\nfor g in range(len(pix_col)):\n    x_train['img'+str(g)] = pix_col[g]\n\n    \nx_train['word_match'] = train_word_match\n\ny_train = s['is_duplicate'].values\npos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this...\np = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train, neg_train])\n    scale -=1\nneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\nprint(len(pos_train) / (len(pos_train) + len(neg_train)))\n\nx_train = pd.concat([pos_train, neg_train])\ny_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\ndel pos_train, neg_train",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 7\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 500, watchlist, early_stopping_rounds=100, verbose_eval=10)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (12.0, 30.0)\nxgb.plot_importance(bst); plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Using this technique and combining it with word match features I got log loss of **0.31858** on test dataset. \n\nI thought this feature can be of some help to others hence shared. Enjoy :)",
      "metadata": {}
    }
  ]
}